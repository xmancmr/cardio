
import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import time

# Configuration de la page
st.set_page_config(
    page_title="Chatbot IA - G√©n√©ration de Texte",
    page_icon="ü§ñ",
    layout="wide"
)

# CSS pour le style du chat
st.markdown("""
<style>
.chat-message {
    padding: 1rem;
    border-radius: 0.5rem;
    margin-bottom: 1rem;
    display: flex;
    flex-direction: column;
}
.chat-message.user {
    background-color: #2b313e;
    align-items: flex-end;
}
.chat-message.bot {
    background-color: #475063;
    align-items: flex-start;
}
.chat-message .avatar {
    width: 3rem;
    height: 3rem;
    border-radius: 50%;
    object-fit: cover;
}
.chat-message .message {
    padding: 0.5rem 1rem;
    border-radius: 0.5rem;
    max-width: 80%;
    word-wrap: break-word;
}
.user-message {
    background-color: #1f4e79;
    color: white;
    margin-left: auto;
}
.bot-message {
    background-color: #2d3748;
    color: white;
    margin-right: auto;
}
</style>
""", unsafe_allow_html=True)

@st.cache_resource
def load_chatbot_model():
    """Charge le mod√®le Phi-3.5-mini-instruct"""
    try:
        with st.spinner("Chargement du mod√®le Phi-3.5-mini-instruct..."):
            model_name = "microsoft/Phi-3.5-mini-instruct"
            
            # Chargement du tokenizer
            tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
            
            # Chargement du mod√®le
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True
            )
            
            # Cr√©ation du pipeline
            text_generator = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None
            )
            
            st.success("‚úÖ Mod√®le charg√© avec succ√®s!")
            return text_generator, tokenizer
            
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement du mod√®le : {str(e)}")
        return None, None

def generate_response(generator, tokenizer, prompt, max_length=512, temperature=0.7):
    """G√©n√®re une r√©ponse avec le mod√®le"""
    try:
        # Format du prompt pour Phi-3.5
        formatted_prompt = f"<|user|>\n{prompt}<|end|>\n<|assistant|>\n"
        
        # G√©n√©ration
        outputs = generator(
            formatted_prompt,
            max_new_tokens=max_length,
            temperature=temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            return_full_text=False
        )
        
        # Extraction de la r√©ponse
        response = outputs[0]['generated_text'].strip()
        
        # Nettoyage de la r√©ponse
        if "<|end|>" in response:
            response = response.split("<|end|>")[0].strip()
            
        return response
        
    except Exception as e:
        return f"Erreur lors de la g√©n√©ration : {str(e)}"

def display_chat_message(message, is_user=False):
    """Affiche un message de chat"""
    if is_user:
        st.markdown(f"""
        <div class="chat-message user">
            <div class="message user-message">
                üë§ {message}
            </div>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="chat-message bot">
            <div class="message bot-message">
                ü§ñ {message}
            </div>
        </div>
        """, unsafe_allow_html=True)

def main():
    st.title("ü§ñ Chatbot IA - G√©n√©ration de Texte")
    st.markdown("**Mod√®le :** Microsoft Phi-3.5-mini-instruct")
    
    # Chargement du mod√®le
    generator, tokenizer = load_chatbot_model()
    
    if generator is None:
        st.error("Impossible de charger le mod√®le. Veuillez r√©essayer.")
        return
    
    # Initialisation de l'historique des conversations
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    # Sidebar avec param√®tres
    with st.sidebar:
        st.header("‚öôÔ∏è Param√®tres de g√©n√©ration")
        
        max_length = st.slider(
            "Longueur maximale", 
            min_value=50, 
            max_value=1000, 
            value=300,
            help="Nombre maximum de tokens √† g√©n√©rer"
        )
        
        temperature = st.slider(
            "Temp√©rature", 
            min_value=0.1, 
            max_value=2.0, 
            value=0.7, 
            step=0.1,
            help="Contr√¥le la cr√©ativit√© (plus √©lev√© = plus cr√©atif)"
        )
        
        st.markdown("---")
        
        # Suggestions de prompts
        st.subheader("üí° Suggestions")
        suggestions = [
            "Explique-moi les maladies cardiovasculaires",
            "Raconte-moi une histoire courte",
            "Comment am√©liorer ma sant√© cardiaque ?",
            "√âcris un po√®me sur la technologie",
            "Donne-moi des conseils nutritionnels"
        ]
        
        for suggestion in suggestions:
            if st.button(suggestion, key=f"suggestion_{suggestion[:20]}"):
                st.session_state.current_input = suggestion
        
        st.markdown("---")
        
        # Bouton pour effacer l'historique
        if st.button("üóëÔ∏è Effacer l'historique"):
            st.session_state.chat_history = []
            st.rerun()
    
    # Zone de chat principale
    st.markdown("### üí¨ Conversation")
    
    # Conteneur pour l'historique des messages
    chat_container = st.container()
    
    with chat_container:
        # Affichage de l'historique
        for message in st.session_state.chat_history:
            display_chat_message(message["content"], message["is_user"])
    
    # Zone de saisie
    st.markdown("---")
    
    # Utilisation du prompt sugg√©r√© s'il existe
    default_input = st.session_state.get("current_input", "")
    if default_input:
        del st.session_state.current_input
    
    # Formulaire de saisie
    with st.form("chat_form", clear_on_submit=True):
        col1, col2 = st.columns([4, 1])
        
        with col1:
            user_input = st.text_area(
                "Votre message :",
                value=default_input,
                placeholder="Tapez votre message ici...",
                height=100,
                key="user_input"
            )
        
        with col2:
            st.write("")  # Espacement
            st.write("")  # Espacement
            send_button = st.form_submit_button("üì§ Envoyer", use_container_width=True)
    
    # Traitement du message
    if send_button and user_input.strip():
        # Ajout du message utilisateur √† l'historique
        st.session_state.chat_history.append({
            "content": user_input,
            "is_user": True
        })
        
        # Affichage du message utilisateur
        display_chat_message(user_input, is_user=True)
        
        # G√©n√©ration de la r√©ponse
        with st.spinner("ü§ñ G√©n√©ration en cours..."):
            response = generate_response(
                generator, 
                tokenizer, 
                user_input, 
                max_length=max_length, 
                temperature=temperature
            )
        
        # Ajout de la r√©ponse √† l'historique
        st.session_state.chat_history.append({
            "content": response,
            "is_user": False
        })
        
        # Affichage de la r√©ponse
        display_chat_message(response, is_user=False)
        
        # Rechargement pour afficher les nouveaux messages
        st.rerun()
    
    # Informations sur le mod√®le
    with st.expander("‚ÑπÔ∏è √Ä propos du mod√®le"):
        st.markdown("""
        **Microsoft Phi-3.5-mini-instruct** est un mod√®le de langage compact mais puissant :
        
        - üîπ **Taille :** 3.8 milliards de param√®tres
        - üîπ **Optimis√© pour :** Conversations et instructions
        - üîπ **Langues :** Multilingue (fran√ßais, anglais, etc.)
        - üîπ **Usage :** G√©n√©ration de texte, Q&A, assistance
        
        Ce mod√®le offre un excellent √©quilibre entre performance et efficacit√© computationnelle.
        """)
    
    # Statistiques de la session
    if st.session_state.chat_history:
        st.markdown("---")
        total_messages = len(st.session_state.chat_history)
        user_messages = len([m for m in st.session_state.chat_history if m["is_user"]])
        bot_messages = total_messages - user_messages
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Messages total", total_messages)
        with col2:
            st.metric("Vos messages", user_messages)
        with col3:
            st.metric("R√©ponses IA", bot_messages)

if __name__ == "__main__":
    main()
